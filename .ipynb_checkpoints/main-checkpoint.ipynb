{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Detection, Allignment, Embeddings, Clustering\n",
    "\n",
    "### Notebook content:\n",
    "- 1 Correct jpg files metadata\n",
    "- 2 MTCNN (detect faces on photos)\n",
    "- 3 Filter results (remove bad quality photos)\n",
    "- 4 FaceNet embeddings (obtain 128-dim vector/embedding representing faces features numerically with FaceNet)\n",
    "- 5 t-SNE: 2-D representation of clustering similar photos together and separating different photos further from each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'piexif'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\OneDrive - Imperial College London\\ML\\_Face\\utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpiexif\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'piexif'"
     ]
    }
   ],
   "source": [
    "# Load libraries and helper functions by running utils.py with a jupyter magic function:\n",
    "%run utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Correct jpg files metadata (creation-time)\n",
    "It is often that metadata for jpg files is wrong (e.g. the camera was not set correctly), here are some usefull functions to check and correct the metadata:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check metadata for jpg files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'print_creation_times_for_subfolders' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-04843d176e7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Scan over immediate subfolders in the main directory folder and check creation times within these subfolders\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr'Test\\Photoset'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint_creation_times_for_subfolders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'print_creation_times_for_subfolders' is not defined"
     ]
    }
   ],
   "source": [
    "# Scan over immediate subfolders in the main directory folder and check creation times within these subfolders\n",
    "directory = r'Test\\Photoset'\n",
    "print_creation_times_for_subfolders(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check creation times in one of the subfolders:\n",
    "subdirectory = r'Test\\Photoset\\2020'\n",
    "get_creation_times_range(subdirectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'Test\\Photoset\\2018\\0.jpeg'\n",
    "get_creation_time(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct metadata (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change single file creation datetime (if needed)\n",
    "file_path_to_change_metadata = r'Test\\Photoset\\2018\\0.jpeg'\n",
    "change_jpg_datetime(file_path_to_change_metadata, 2018, 9, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change creation times of all jpg files in a folder to specific date\n",
    "folder_to_change_metadata = r'Test\\Photoset\\2020'\n",
    "change_datetime_in_folder(folder_to_change_metadata, 2020, 6, 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. MTCNN (Multi-Task CNN)\n",
    "Multi Task CNN performs multiple tasks simultaneously: face detection and face allignment - i.e. finding face box and landmarks (coordinates of eyes, nose, mouth edges) on the face.\n",
    "- Original paper (2016): https://arxiv.org/abs/1604.02878  \n",
    "- Github repo: https://github.com/ipazc/mtcnn by https://www.linkedin.com/in/ivandepazcenteno/:   \n",
    "- Description with examples https://machinelearningmastery.com/how-to-perform-face-detection-with-classical-and-deep-learning-methods-in-python-with-keras/  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan a folder with subfolders containing photos and perfrom MTCNN on all photos, save results to a csv file\n",
    "# Depending on the archive size, scanning photos might take a lot of time.\n",
    "# The results are saved to the MTCNN_results file for each photo scanned.\n",
    "# Thus, if interrupted, the results are still saved, and the scan can continued later.\n",
    "# If new photos are added to the archive this function will do MTCNN only for new photos.\n",
    "# If one wants a completly new scan with different parameters (min_face_size) new MTCNN_results csv file should be created\n",
    "\n",
    "# Main archive photo folder with subfolders containing photos\n",
    "photo_folder = r'Test\\Photoset'\n",
    "# csv file to save scan results\n",
    "MTCNN_results_file_path = r'Test\\MTCNN_results.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run MTCNN and save results:\n",
    "get_mtcnn_results(photo_folder, MTCNN_results_file_path, min_face_size = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Filter photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_MTCNN_scan_results(MTCNN_results_file_path)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MTCNN results need filtering for several reasons:\n",
    "E.g: Not a face detected, face size is too small for further processing, photo not in front (person looks to the side), face not in focus, face is inclined too much, grayscale image, etc...\n",
    "\n",
    "### Define the filters:\n",
    "- **confidence_filter** allows to filter out photos with low confidence. MTCNN provides its confidence in a found face as one of it output results. Low confidence values correspond to either not-a-face (objects resembling face) or occluded faces - it is better to filter them out, e.g. faces with confidence less than 0.99 will be excluded.\n",
    "\n",
    "- **face_height_filter**. Faces with height less than face_height_filter will be excluded.  Although MTCNN takes min_face_size as an argument some of the found faces are smaller than the min_face_size parameter.\n",
    "\n",
    "- **nose_shift_filter**. If nose shift is bigger than e.g 15 - face will be excluded. I tried to filter out the photos which are not in front. Nose shift is determined from positions of eyes and nose landmarks and if the nose is strongly shifted relative to center between the eyes the photo is filtered out. \n",
    "\n",
    "- **eye_line_angle_filter**. If eye_line_angle is more than eye_line_angle_filter - face will be excluded. All photos will be rotated so that the eyeline is horizontal.\n",
    "\n",
    "- **sharpness_filter**. Assessment of bluriness of an image. Bigger value - lower bluriness. Idea taken from https://www.pyimagesearch.com/2015/09/07/blur-detection-with-opencv/ and modified. I found it emperically that (max-min)**2/var is working better than simple variance as an assessment of bluriness, it is also implemented on a central part of the face around nose (not on the whole face image). If sharpness is less than sharpness_filter face will be excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the filters:\n",
    "confidence_filter = 0.98\n",
    "face_height_filter = 10\n",
    "nose_shift_filter = 25\n",
    "eye_line_angle_filter = 45\n",
    "sharpness_filter = 20\n",
    "# grayscale_image_filter is on by default, so no grayscale images will pass through\n",
    "\n",
    "# And preview the images (one can find appropriate filters depending on the task)\n",
    "# The landmarks on faces are shown in preview mode and are not saved in the save mode\n",
    "save_image_folder = False\n",
    "preview = True\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (3,3)\n",
    "image_filter(df[0:100], save_image_folder, preview, confidence_filter, face_height_filter, nose_shift_filter, eye_line_angle_filter, sharpness_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Now that we decided on filter parameters for the task, let's get and save the filtered results:\n",
    "save_image_folder = r'Test\\Cropped_face_images'\n",
    "preview = False\n",
    "\n",
    "df_filtered = image_filter(df, save_image_folder, preview, confidence_filter, face_height_filter, nose_shift_filter, eye_line_angle_filter, sharpness_filter)\n",
    "\n",
    "#file_path_filtered_results = r'output\\MTCNN_min_face_200_filtered.csv'\n",
    "file_path_filtered_results = r'Test\\MTCNN_results_filtered.csv'\n",
    "\n",
    "print('Number of faces before filtering:', len(df), 'Number of faces after filtering:', len(df_filtered))\n",
    "df_filtered.to_csv(file_path_filtered_results,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_filtered = pd.read_csv(r'output\\MTCNN_min_face_200_filtered.csv')\n",
    "df_filtered = pd.read_csv(file_path_filtered_results)\n",
    "print(len(df_filtered))\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. FaceNet Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pretrained facenet model\n",
    "# The model is taking 160X160 colored figure as input \n",
    "# And outputs 128 value vector of embedding generated from this figure\n",
    "model = load_model(r'facenet_keras_pretrained/model/facenet_keras.h5')\n",
    "print('model_input:', model.inputs)\n",
    "print('model_output:', model.outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Get single embedding\n",
    "image_path = r'Test\\Cropped_face_images\\0.jpg'\n",
    "emb = get_facenet_embedding(image_path, model)\n",
    "print(emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Get embeddings for all face images\n",
    "embeddings = []\n",
    "for file_path in tqdm(df_filtered['face_file_path']):\n",
    "    emb = get_facenet_embedding(file_path, model)\n",
    "    embeddings.append(emb)\n",
    "embeddings = np.array(embeddings)\n",
    "#print(embeddings.shape)\n",
    "\n",
    "# Save embeddings to the file\n",
    "embeddings_file_path = r'Test\\embeddings'\n",
    "np.save(embeddings_file_path, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load filetered results\n",
    "df_filtered = pd.read_csv(file_path_filtered_results)\n",
    "\n",
    "# load the embeddings\n",
    "X = np.load('Test\\embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate t-SNE\n",
    "X_tsne = TSNE(perplexity=2, learning_rate = 1000, n_iter=1000, random_state=0).fit_transform(X)\n",
    "\n",
    "x = X_tsne[:,0]\n",
    "y = X_tsne[:,1]\n",
    "paths =  df_filtered.face_file_path\n",
    "creation_dates = df_filtered.creation_date\n",
    "\n",
    "\n",
    "# The idea on how to plot faces is taken from https://stackoverflow.com/questions/22566284/matplotlib-how-to-plot-images-instead-of-points\n",
    "def getImage(path, size):\n",
    "    image = plt.imread(path)\n",
    "    image = resize_image(image, size)\n",
    "    return OffsetImage(image)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
    "# This part is plotting faces\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y)\n",
    "for x0, y0, path in zip(x, y, paths):\n",
    "    ab = AnnotationBbox(getImage(path, (50,50)), (x0, y0), frameon=False)\n",
    "    ax.add_artist(ab)\n",
    "\n",
    "# This part is plotting colors\n",
    "# Creation date is shown with colors on the plot below\n",
    "# Spectral(rainbow) palette is used with older photos shown in red and recent in blue\n",
    "sns.scatterplot(x=x, y=y, hue = creation_dates, s=7000, palette=sns.color_palette('Spectral',len(set(creation_dates))))\n",
    "#plt.legend([],[], frameon=False) # hide the legend if it too long\n",
    "\n",
    "# Set figure background\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False,'axes.facecolor': 'white'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see t-SNE separated these photos to groups using face embeddings generated by FaceNet. t-SNE is not ideal (one photo of my daughter mixed with my son's phots on this figure for example), and the result depends on the hyperparameters of the t-SNE algorithm. It also initialize randomly, so changing random_state brings different results. The output becomes more robust and accurate if more photos is used as is shown below for 1700 facial images from my archive. On this figure I can clearly members of our family and friends nicely separated to groups.<img src=\"Test/all.png\">\n",
    "# Do try it for your photo archive!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
